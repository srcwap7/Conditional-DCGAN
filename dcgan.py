# -*- coding: utf-8 -*-
"""DCGAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18fZ372d95VlBhM9Pq6ZZcRJRKnj6OLD4
"""

!pip install torch torchvision

import torch
import os
print (torch.__version__)
print (torch.cuda.is_available())
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

!export CUDA_LAUNCH_BLOCKING=1

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np

# First ensure all required torch modules are imported
from torch.nn import ConvTranspose2d, BatchNorm2d, ReLU, Tanh
from torch.nn import Conv2d, LeakyReLU, Linear, Sigmoid

class Generator(nn.Module):
    def __init__(self, latent_dim=100, out_channel=3, num_classes=100):
        super(Generator, self).__init__()
        self.latent_dim = latent_dim

        self.label_emb = nn.Embedding(num_classes, latent_dim)

        # Initial size: (2*latent_dim, 1, 1)
        self.gen = nn.Sequential(
            # Size: (2*latent_dim, 1, 1) -> (128, 4, 4)
            nn.ConvTranspose2d(2*latent_dim, 128, kernel_size=4, stride=2, padding=0),
            nn.BatchNorm2d(128),
            nn.ReLU(True),

            # Size: (128, 4, 4) -> (64, 8, 8)
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(True),

            # Size: (64, 8, 8) -> (32, 16, 16)
            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(True),

            # Size: (32, 16, 16) -> (16, 32, 32)
            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(True),

            # Size: (16, 32, 32) -> (out_channel, 64, 64)
            nn.ConvTranspose2d(16, out_channel, kernel_size=4, stride=2, padding=1),
            nn.Tanh()
        )

    def forward(self, x, labels):
        # Reshape input: (batch_size, latent_dim) -> (batch_size, latent_dim, 1, 1)
        x = x.view(x.size(0), self.latent_dim, 1, 1)

        # Get label embeddings and reshape
        label_embedding = self.label_emb(labels)
        label_embedding = label_embedding.view(label_embedding.size(0), self.latent_dim, 1, 1)

        # Concatenate noise and label embedding
        x = torch.cat((x, label_embedding), dim=1)

        # Generate image
        return self.gen(x)

class Discriminator(nn.Module):
    def __init__(self, in_channels=3, num_classes=100, alpha=0.2):
        super(Discriminator, self).__init__()

        self.label_emb = nn.Embedding(num_classes, in_channels)

        self.disc = nn.Sequential(
            # Input: (in_channels * 2, 64, 64)
            nn.Conv2d(in_channels * 2, 16, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(alpha),

            # (16, 32, 32)
            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(alpha),

            # (32, 16, 16)
            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(alpha),

            # (64, 8, 8)
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(alpha),
            # Output: (128, 4, 4)
        )

        self.classifier = nn.Sequential(
            nn.Linear(128 * 4 * 4, 1),
            nn.Sigmoid()
        )

    def forward(self, x, labels):
        batch_size = x.size(0)

        # Get label embedding and reshape
        label_embedding = self.label_emb(labels)
        label_embedding = label_embedding.view(batch_size, -1, 1, 1)
        label_embedding = label_embedding.repeat(1, 1, x.size(2), x.size(3))

        # Concatenate image and label embedding
        x = torch.cat([x, label_embedding], dim=1)

        # Pass through convolutional layers
        x = self.disc(x)

        # Flatten and classify
        x = x.view(batch_size, -1)
        return self.classifier(x)

# Setup parameters
batch_size = 64
in_channels = 3
num_classes = 100
latent_dim = 100
learning_rate = 0.0002
beta1 = 0.5
num_epochs = 100

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Setup data transforms
transform = transforms.Compose([
    transforms.Resize(64),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load dataset
dataset = datasets.CIFAR100(root='./data', train=True, transform=transform, download=True)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Initialize models
generator = Generator(latent_dim=latent_dim, out_channel=in_channels, num_classes=num_classes)
discriminator = Discriminator(in_channels=in_channels, num_classes=num_classes, alpha=0.2)

# Move models to device
generator = generator.to(device)
discriminator = discriminator.to(device)

# Initialize optimizers
g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate, betas=(beta1, 0.999))
d_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(beta1, 0.999))

# Loss function
criterion = nn.BCELoss()

print("Setup complete!")

# Test the models with a small batch
test_noise = torch.randn(batch_size, latent_dim).to(device)
test_labels = torch.randint(0, num_classes, (batch_size,)).to(device)
try:
    test_output = generator(test_noise, test_labels)
    print(f"Generator output shape: {test_output.shape}")

    test_disc = discriminator(test_output, test_labels)
    print(f"Discriminator output shape: {test_disc.shape}")
    print("Model test successful!")
except Exception as e:
    print(f"Error during model test: {str(e)}")

